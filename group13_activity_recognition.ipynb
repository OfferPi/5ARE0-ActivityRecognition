{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c51ad9",
   "metadata": {},
   "source": [
    "# Human Activity Recognition – Assignment 1  \n",
    "### 5ARE0, Academic Year 2025–2026  \n",
    "\n",
    "**Group 13**  \n",
    "\n",
    "**Contributors:**  \n",
    "- Stan Lamerikx\n",
    "- Simon Lammertink\n",
    "- Philip Offermans\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cedcfa",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf436c6",
   "metadata": {},
   "source": [
    "Below all relevant packages which are used throughout the notebook are imported. The required packages are the same as the ones used throughout the course 5ARE0. When the setup is properly carried out according to the README.md file, all of these packages should be present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "81baf178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Dict, List, Optional, Tuple, Union\n",
    "from __future__ import annotations\n",
    "from typing import Dict, List\n",
    "from scipy.signal import butter, filtfilt\n",
    "from collections import Counter\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e5aa5",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34e5fc2",
   "metadata": {},
   "source": [
    "Below, the zip_files in the data folder are extracted. For each of the actions, a seperate directory containing the data from the sensor logger app is created. This leads to four folders:\n",
    "- walking\n",
    "- running \n",
    "- climbing stairs\n",
    "- sitting down + standing up (these actions are combined in one recording, to be split later)\n",
    "\n",
    "Of each of these folders, three instances exist, one for each person in the group. The folders are noted with numbers 1, 2 and 3 to preserve anonimity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "c73f6ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted ./data\\climbingStairs_1.zip to ./data\\climbingStairs_1\n",
      "Extracted ./data\\climbingStairs_2.zip to ./data\\climbingStairs_2\n",
      "Extracted ./data\\climbingStairs_3.zip to ./data\\climbingStairs_3\n",
      "Extracted ./data\\running_1.zip to ./data\\running_1\n",
      "Extracted ./data\\running_2.zip to ./data\\running_2\n",
      "Extracted ./data\\running_3.zip to ./data\\running_3\n",
      "Extracted ./data\\sittingDown+StandingUp_1.zip to ./data\\sittingDown+StandingUp_1\n",
      "Extracted ./data\\sittingDown+StandingUp_2.zip to ./data\\sittingDown+StandingUp_2\n",
      "Extracted ./data\\sittingDown+StandingUp_3.zip to ./data\\sittingDown+StandingUp_3\n",
      "Extracted ./data\\walking_1.zip to ./data\\walking_1\n",
      "Extracted ./data\\walking_2.zip to ./data\\walking_2\n",
      "Extracted ./data\\walking_3.zip to ./data\\walking_3\n"
     ]
    }
   ],
   "source": [
    "zip_folder = \"./data\"   #Find data folder\n",
    "\n",
    "data_dirs = []\n",
    "# Loop through all files in the folder\n",
    "for file in os.listdir(zip_folder):\n",
    "    if file.endswith(\".zip\"):\n",
    "        zip_path = os.path.join(zip_folder, file)\n",
    "        extract_dir = os.path.splitext(zip_path)[0]  # remove .zip\n",
    "        \n",
    "        # Add folder to list of folders with data\n",
    "        data_dirs.append(extract_dir)\n",
    "\n",
    "        # Extract\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_dir)\n",
    "\n",
    "        print(f\"Extracted {zip_path} to {extract_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af851fb",
   "metadata": {},
   "source": [
    "The data provided by the sensor logger app is presented in a comma-seperated value file (.csv). The function below converts the files into a dataframe. In addition, the column for 'time' is dropped, since we will be using the column 'seconds_elapsed' instead as a time indication. Finally, as a check, missing values are interpolated in this function; though in practice no missing values happen to be present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "91b1de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _csv_to_df(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read CSV and return as DataFrame.\n",
    "    Remove the column \"time\"\n",
    "    Check for missing values and interpolate\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to dataframe\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Remove time column \n",
    "    if \"time\" in df.columns:\n",
    "        df = df.drop(columns=[\"time\"])\n",
    "\n",
    "    # Check and interpolate missing values\n",
    "    if df.isna().any().any():\n",
    "        print(\"MISSING VALUE\")\n",
    "        df = df.interpolate(method=\"linear\").dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486d8aab",
   "metadata": {},
   "source": [
    "## Initialize data and cut to desired timeframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d7717e",
   "metadata": {},
   "source": [
    "As can also be read in the documentation report, to ensure easy data pre-processing, the data measurement started at the latest 25 seconds after beginning the recording and ended at the earliest 295 seconds after starting the recording. This means that by consistently cutting between these known points, a dataframe containing a continuous data set can be created.\n",
    "\n",
    "To achieve this, a function is created below to cut dataframes between an input start and end time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d9d35b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cut(df: pd.DataFrame, t_min: float, t_max: float) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Limit dataframes to selected times\n",
    "    Reset beginning to t = 0s\n",
    "\n",
    "        Parameters: \n",
    "                t_min: start time\n",
    "                t_max: end time\n",
    "\n",
    "    \"\"\"\n",
    "    cut = df[(df[\"seconds_elapsed\"] >= t_min) & (df[\"seconds_elapsed\"] <= t_max)].copy()\n",
    "    cut.sort_values(\"seconds_elapsed\", inplace=True, kind=\"stable\")\n",
    "    cut.reset_index(drop=True, inplace=True)\n",
    "    cut[\"seconds_elapsed\"] = cut[\"seconds_elapsed\"] - cut[\"seconds_elapsed\"].iloc[0]\n",
    "\n",
    "    return cut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c16fd6",
   "metadata": {},
   "source": [
    "To correctly extract only the data which we want to use, a list of strings is defined for both the sensors and actions which includes all desired data. This makes sure that, for example, the uncalibrated date is not also present in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d5649ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define relevant sensor data\n",
    "SENSORS: list[str] = [\n",
    "    \"Accelerometer\",\n",
    "    \"Gyroscope\",\n",
    "    \"Gravity\",\n",
    "]\n",
    "\n",
    "#Define list of actions\n",
    "ACTIONS: list[str] = [\n",
    "    \"climbingStairs\",\n",
    "    \"running\",\n",
    "    \"sittingDown+StandingUp\",\n",
    "    \"walking\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab91d01a",
   "metadata": {},
   "source": [
    "Finally, the function below uses the lists of sensors and action, as well as the function used to cut the data to create data frames which are ready for pre-processing. Notice in the code that a slightly different approach is taken for the sitting down and standing up data, since these are present in a single recording, and should thus be cut accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "78d3a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sensors: list[str], actions: list[str], location: str = \"./data/{action}*/\") -> dict:\n",
    "    \"\"\"\n",
    "    Limit data to given lists of sensors and actions.\n",
    "    Cut all resulting data between relevant times.\n",
    "\n",
    "    \"\"\"\n",
    "    # Create empty dict to store data\n",
    "    cut_data = {}\n",
    "\n",
    "    # Go trough all actions\n",
    "    for action in actions:\n",
    "        # find all recordings for an action\n",
    "        rec_dirs = [d for d in glob.glob(location.format(action=action))]\n",
    "\n",
    "        # Set the cut moment for the sitting down and standing up action\n",
    "        # This end time is later, given that actions are combined into single recording\n",
    "        # For more details regarding these times, check recording methodology in report\n",
    "        if action == \"sittingDown+StandingUp\":\n",
    "            t_min, t_max = 25.0, 565.1\n",
    "\n",
    "        # Set begin and end time for all other actions\n",
    "        # Times correspond to recording methodology discussed in report\n",
    "        else:\n",
    "            t_min, t_max = 25.0, 295.1\n",
    "\n",
    "        \n",
    "        cut_data[action] = {}\n",
    "\n",
    "        # Go trough all recordings\n",
    "        for rec in rec_dirs:\n",
    "            # Go trough all sensors\n",
    "            for sensor in sensors:\n",
    "                # Create location of csv\n",
    "                csv_path = f\"{rec}{sensor}.csv\"\n",
    "\n",
    "                # Use function created before to convert to dataframe\n",
    "                df = _csv_to_df(csv_path)\n",
    "\n",
    "                # Use function before to cut dataframe to appropriate begin and end time\n",
    "                df_cut = _cut(df, t_min, t_max)\n",
    "                \n",
    "                # Initialize list if it doesn't exist\n",
    "                if sensor not in cut_data[action]:\n",
    "                    cut_data[action][sensor] = [df_cut]\n",
    "                else:\n",
    "                    cut_data[action][sensor].append(df_cut) \n",
    "    return cut_data\n",
    "\n",
    "data_raw = load_data(sensors=SENSORS, actions=ACTIONS, location=\"./data/{action}*/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788cd8d0",
   "metadata": {},
   "source": [
    "## Filter / noise reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f32d77d",
   "metadata": {},
   "source": [
    "Given the data initialized in the previous step, some initial pre-processing can be performed. For these purposes, a low pass filter will be applied to filter out higher-frequency noise. Continuing, a moving average filter can be applied as a form of data aggregation.\n",
    "\n",
    "Below, two general functions are given for a low pass filter and a moving average filter. These are in correspondance with the methodology supplied in the instructions for 5ARE0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94965725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_pass_filter(series, cutoff_hz: float, fs: float, order: int = 4):\n",
    "    \"\"\"\n",
    "    filter to cut off frequencies above a certain threshold\n",
    "\n",
    "        Parameters:\n",
    "                cutoff_hz: cutoff frequency (Hz)\n",
    "                fs: sample rate (Hz)\n",
    "\n",
    "    \"\"\"\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff_hz / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype=\"low\", analog=False)\n",
    "    return filtfilt(b, a, series.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51529e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(series, window_size: int = 5):\n",
    "    \"\"\"\"\n",
    "    Compute a centered moving average of input data\n",
    "     \n",
    "    \"\"\"\n",
    "\n",
    "    return series.rolling(window=window_size, center=True, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd32a48",
   "metadata": {},
   "source": [
    "To properly apply the filters to all data, a seperate function is written which iterates over:\n",
    "\n",
    "- all actions in the 'action list'\n",
    "- all sensors in the 'sensor list'\n",
    "- all axes in the data (x, y and z)\n",
    "\n",
    "The function then applies the low pass and moving average filter to each.\n",
    "\n",
    "This function is immediately applied unto the data, with the input sampling rate as 100 Hz (in correspondance with the metadata) and a cutoff frequency of 20 Hz (experimentally decided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "8ae8b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_low_pass_filter(data: dict, fs: float, cutoff: float) -> dict:\n",
    "    \"\"\"\n",
    "    Apply low pass filter\n",
    "\n",
    "    \"\"\"\n",
    "    filt_data = {}\n",
    "    for action, sensors in data.items():\n",
    "        filt_data[action] = {}\n",
    "        for sensor, recordings in sensors.items():\n",
    "            filt_data[action][sensor] = []\n",
    "            for df in recordings:\n",
    "                df_filt = df.copy()\n",
    "                for axis in [\"x\", \"y\", \"z\"]:\n",
    "                    df_filt[axis] = low_pass_filter(df[axis], cutoff_hz=cutoff, fs=fs)\n",
    "                filt_data[action][sensor].append(df_filt)\n",
    "    return filt_data\n",
    "\n",
    "def process_moving_average(data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Apply moving average\n",
    "    \n",
    "    \"\"\"\n",
    "    filt_data = {}\n",
    "    for action, sensors in data.items():\n",
    "        filt_data[action] = {}\n",
    "        for sensor, recordings in sensors.items():\n",
    "            filt_data[action][sensor] = []\n",
    "            for df in recordings:\n",
    "                df_filt = df.copy()\n",
    "                for axis in [\"x\", \"y\", \"z\"]:\n",
    "                    df_filt[axis] = moving_average(df[axis], window_size=5)\n",
    "                filt_data[action][sensor].append(df_filt)\n",
    "    return filt_data\n",
    "\n",
    "data = process_moving_average(process_low_pass_filter(data_raw,100,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bdbfd2",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "7e88a540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x15b9c4d9ac0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Dash visualizer with smoothing =========================================\n",
    "# Expects: data[action][sensor] -> list[pd.DataFrame] with 'seconds_elapsed' + x/y/z numeric cols\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from dash import Dash, dcc, html, Input, Output, State, no_update\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Optional: SciPy for Butterworth + Savitzky–Golay\n",
    "try:\n",
    "    from scipy.signal import butter, filtfilt, savgol_filter\n",
    "    SCIPY_OK = True\n",
    "except Exception:\n",
    "    SCIPY_OK = False\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def list_actions(data: dict) -> List[str]:\n",
    "    return sorted([a for a in data.keys() if isinstance(data[a], dict)])\n",
    "\n",
    "def list_sensors_for_action(data: dict, action: str) -> List[str]:\n",
    "    if action not in data or not isinstance(data[action], dict):\n",
    "        return []\n",
    "    sensors = []\n",
    "    for s, dfs in data[action].items():\n",
    "        valid = any(isinstance(df, pd.DataFrame) and not df.empty for df in (dfs or []))\n",
    "        if valid:\n",
    "            sensors.append(s)\n",
    "    return sorted(sensors)\n",
    "\n",
    "def list_recording_indices(data: dict, action: str, sensor: str) -> List[int]:\n",
    "    if action not in data or sensor not in data[action]:\n",
    "        return []\n",
    "    return [i for i, df in enumerate(data[action][sensor]) if isinstance(df, pd.DataFrame) and not df.empty]\n",
    "\n",
    "def available_axes(df: pd.DataFrame) -> List[str]:\n",
    "    axes = [c for c in [\"x\", \"y\", \"z\"] if c in df.columns]\n",
    "    if axes:\n",
    "        return axes\n",
    "    return [c for c in df.select_dtypes(\"number\").columns if c != \"seconds_elapsed\"]\n",
    "\n",
    "def infer_fs(df: pd.DataFrame) -> float | None:\n",
    "    if \"seconds_elapsed\" not in df.columns:\n",
    "        return None\n",
    "    t = df[\"seconds_elapsed\"].to_numpy()\n",
    "    dt = np.median(np.diff(t))\n",
    "    if not np.isfinite(dt) or dt <= 0:\n",
    "        return None\n",
    "    return 1.0 / dt\n",
    "\n",
    "def moving_average(x: np.ndarray, window: int) -> np.ndarray:\n",
    "    if window <= 1:\n",
    "        return x\n",
    "    # symmetric centered window via convolution; pad to keep length\n",
    "    pad = window // 2\n",
    "    xpad = np.pad(x, (pad, pad), mode=\"edge\")\n",
    "    kernel = np.ones(window) / window\n",
    "    y = np.convolve(xpad, kernel, mode=\"valid\")\n",
    "    return y\n",
    "\n",
    "def apply_smoothing(df: pd.DataFrame, axes: List[str], method: str, params: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a new DataFrame with same columns; selected axes replaced or added with smoothed data.\n",
    "    If params.get(\"replace\") is False, smoothed series are added as '<axis>_sm'.\n",
    "    Supported methods: none, ma, sg, lp, hp\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    if method == \"none\" or not axes:\n",
    "        return out\n",
    "\n",
    "    replace = bool(params.get(\"replace\", False))\n",
    "    # Downsample occurs outside; we smooth current df slice\n",
    "    if method == \"ma\":\n",
    "        window = int(params.get(\"ma_window\", 5))\n",
    "        window = max(1, window | 1)  # force odd\n",
    "        for a in axes:\n",
    "            if a in out.columns:\n",
    "                y = moving_average(out[a].to_numpy(), window)\n",
    "                out[a if replace else f\"{a}_sm\"] = y\n",
    "\n",
    "    elif method == \"sg\" and SCIPY_OK:\n",
    "        window = int(params.get(\"sg_window\", 7))\n",
    "        poly = int(params.get(\"sg_poly\", 3))\n",
    "        window = max(poly + 2 | 1, window | 1)  # ensure odd & >= poly+2\n",
    "        for a in axes:\n",
    "            if a in out.columns and len(out[a]) >= window:\n",
    "                y = savgol_filter(out[a].to_numpy(), window_length=window, polyorder=poly, mode=\"interp\")\n",
    "                out[a if replace else f\"{a}_sm\"] = y\n",
    "\n",
    "    elif method in (\"lp\", \"hp\") and SCIPY_OK:\n",
    "        fs = infer_fs(out)\n",
    "        if not fs:\n",
    "            return out\n",
    "        cutoff = float(params.get(\"bw_cutoff\", min(fs * 0.45, fs/2 - 1e-3)))\n",
    "        order = int(params.get(\"bw_order\", 4))\n",
    "        nyq = 0.5 * fs\n",
    "        wc = np.clip(cutoff / nyq, 1e-6, 0.999999)\n",
    "        btype = \"low\" if method == \"lp\" else \"high\"\n",
    "        b, a = butter(order, wc, btype=btype)\n",
    "        for a_col in axes:\n",
    "            if a_col in out.columns and len(out[a_col]) > order * 3:\n",
    "                y = filtfilt(b, a, out[a_col].to_numpy(), method=\"gust\")\n",
    "                out[a_col if replace else f\"{a_col}_sm\"] = y\n",
    "    # If SciPy missing for sg/lp/hp, we silently leave data unchanged (raw or MA still works)\n",
    "    return out\n",
    "\n",
    "# ---------- App ----------\n",
    "app = Dash(__name__)\n",
    "app.title = \"Recording Visualizer\"\n",
    "\n",
    "_actions = list_actions(data)\n",
    "_sensors = list_sensors_for_action(data, _actions[0]) if _actions else []\n",
    "_recs    = list_recording_indices(data, _actions[0], _sensors[0]) if (_actions and _sensors) else []\n",
    "_rec_val = _recs[0] if _recs else None\n",
    "\n",
    "app.layout = html.Div(\n",
    "    style={\"maxWidth\": \"1180px\", \"margin\": \"0 auto\", \"fontFamily\": \"Inter, system-ui, sans-serif\"},\n",
    "    children=[\n",
    "        html.H2(\"Recordings viewer\", style={\"marginTop\": \"16px\"}),\n",
    "\n",
    "        # Row 1: dataset selectors\n",
    "        html.Div(\n",
    "            style={\"display\": \"grid\", \"gridTemplateColumns\": \"1fr 1fr 1fr\", \"gap\": \"12px\"},\n",
    "            children=[\n",
    "                html.Div([\n",
    "                    html.Label(\"Action\"),\n",
    "                    dcc.Dropdown(\n",
    "                        id=\"dd-action\",\n",
    "                        options=[{\"label\": a, \"value\": a} for a in _actions],\n",
    "                        value=_actions[0] if _actions else None,\n",
    "                        clearable=False,\n",
    "                    )\n",
    "                ]),\n",
    "                html.Div([\n",
    "                    html.Label(\"Sensor\"),\n",
    "                    dcc.Dropdown(\n",
    "                        id=\"dd-sensor\",\n",
    "                        options=[{\"label\": s, \"value\": s} for s in _sensors],\n",
    "                        value=_sensors[0] if _sensors else None,\n",
    "                        clearable=False,\n",
    "                    )\n",
    "                ]),\n",
    "                html.Div([\n",
    "                    html.Label(\"Recording index\"),\n",
    "                    dcc.Dropdown(\n",
    "                        id=\"dd-rec\",\n",
    "                        options=[{\"label\": str(i), \"value\": i} for i in _recs],\n",
    "                        value=_rec_val,\n",
    "                        clearable=False,\n",
    "                    )\n",
    "                ]),\n",
    "            ],\n",
    "        ),\n",
    "\n",
    "        # Row 2: plot options\n",
    "        html.Div(\n",
    "            style={\"display\": \"grid\", \"gridTemplateColumns\": \"1fr 1fr 1fr\", \"gap\": \"12px\", \"marginTop\": \"12px\"},\n",
    "            children=[\n",
    "                html.Div([\n",
    "                    html.Label(\"Axes to plot\"),\n",
    "                    dcc.Checklist(\n",
    "                        id=\"cl-axes\",\n",
    "                        options=[],  # filled dynamically\n",
    "                        value=[],\n",
    "                        inputStyle={\"marginRight\": \"6px\", \"marginLeft\": \"12px\"},\n",
    "                        inline=True,\n",
    "                    ),\n",
    "                ]),\n",
    "                html.Div([\n",
    "                    html.Label(\"Downsample (every Nth point)\"),\n",
    "                    dcc.Slider(\n",
    "                        id=\"sl-downsample\",\n",
    "                        min=1, max=20, step=1, value=1,\n",
    "                        marks={1: \"1x\", 5: \"5x\", 10: \"10x\", 20: \"20x\"},\n",
    "                        tooltip={\"placement\":\"bottom\"}\n",
    "                    ),\n",
    "                ]),\n",
    "                html.Div([\n",
    "                    html.Label(\"Show mode\"),\n",
    "                    dcc.RadioItems(\n",
    "                        id=\"ri-showmode\",\n",
    "                        options=[\n",
    "                            {\"label\": \"Raw only\", \"value\": \"raw\"},\n",
    "                            {\"label\": \"Smoothed only\", \"value\": \"sm\"},\n",
    "                            {\"label\": \"Overlay\", \"value\": \"overlay\"},\n",
    "                        ],\n",
    "                        value=\"raw\",\n",
    "                        inline=True,\n",
    "                    ),\n",
    "                ]),\n",
    "            ]\n",
    "        ),\n",
    "\n",
    "        # Row 3: smoothing controls\n",
    "        html.Div(\n",
    "            style={\"display\": \"grid\", \"gridTemplateColumns\": \"1.2fr 1fr 1fr 1fr 1fr\", \"gap\": \"12px\", \"marginTop\": \"12px\"},\n",
    "            children=[\n",
    "                html.Div([\n",
    "                    html.Label(\"Smoothing method\"),\n",
    "                    dcc.Dropdown(\n",
    "                        id=\"dd-smooth\",\n",
    "                        options=[\n",
    "                            {\"label\": \"None\", \"value\": \"none\"},\n",
    "                            {\"label\": \"Moving Average\", \"value\": \"ma\"},\n",
    "                            {\"label\": \"Savitzky–Golay\", \"value\": \"sg\"},\n",
    "                            {\"label\": \"Butterworth Low-pass\", \"value\": \"lp\"},\n",
    "                            {\"label\": \"Butterworth High-pass\", \"value\": \"hp\"},\n",
    "                        ],\n",
    "                        value=\"none\",\n",
    "                        clearable=False,\n",
    "                    )\n",
    "                ]),\n",
    "                html.Div([\n",
    "                    html.Label(\"MA / SG window\"),\n",
    "                    dcc.Input(id=\"in-ma-sg-window\", type=\"number\", value=7, min=1, step=2, style={\"width\":\"100%\"}),\n",
    "                ]),\n",
    "                html.Div([\n",
    "                    html.Label(\"SG polyorder\"),\n",
    "                    dcc.Input(id=\"in-sg-poly\", type=\"number\", value=3, min=1, step=1, style={\"width\":\"100%\"}),\n",
    "                ]),\n",
    "                html.Div([\n",
    "                    html.Label(\"BW cutoff (Hz)\"),\n",
    "                    dcc.Input(id=\"in-bw-cutoff\", type=\"number\", value=2.0, min=0.01, step=0.1, style={\"width\":\"100%\"}),\n",
    "                ]),\n",
    "                html.Div([\n",
    "                    html.Label(\"BW order\"),\n",
    "                    dcc.Input(id=\"in-bw-order\", type=\"number\", value=4, min=1, step=1, style={\"width\":\"100%\"}),\n",
    "                ]),\n",
    "            ]\n",
    "        ),\n",
    "\n",
    "        html.Div(\n",
    "            style={\"marginTop\": \"6px\", \"fontSize\": \"12px\", \"opacity\": 0.75},\n",
    "            children=(\"Tip: Window must be odd. For SG, window ≥ polyorder+2. \"\n",
    "                      \"For Butterworth, cutoff must be < Nyquist (fs/2).\")\n",
    "        ),\n",
    "\n",
    "        dcc.Graph(id=\"graph\", figure=go.Figure(), style={\"height\": \"70vh\", \"marginTop\": \"8px\"}),\n",
    "\n",
    "        # Stores for preserving UI state\n",
    "        dcc.Store(id=\"xrange-store\"),\n",
    "        dcc.Store(id=\"current-df-meta\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ---------- Callbacks ----------\n",
    "@app.callback(\n",
    "    Output(\"dd-sensor\", \"options\"),\n",
    "    Output(\"dd-sensor\", \"value\"),\n",
    "    Input(\"dd-action\", \"value\"),\n",
    ")\n",
    "def on_action_change(action):\n",
    "    if not action:\n",
    "        return [], None\n",
    "    sensors = list_sensors_for_action(data, action)\n",
    "    opts = [{\"label\": s, \"value\": s} for s in sensors]\n",
    "    val = sensors[0] if sensors else None\n",
    "    return opts, val\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"dd-rec\", \"options\"),\n",
    "    Output(\"dd-rec\", \"value\"),\n",
    "    Input(\"dd-action\", \"value\"),\n",
    "    Input(\"dd-sensor\", \"value\"),\n",
    ")\n",
    "def on_sensor_change(action, sensor):\n",
    "    if not action or not sensor:\n",
    "        return [], None\n",
    "    recs = list_recording_indices(data, action, sensor)\n",
    "    opts = [{\"label\": str(i), \"value\": i} for i in recs]\n",
    "    val = recs[0] if recs else None\n",
    "    return opts, val\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"cl-axes\", \"options\"),\n",
    "    Output(\"cl-axes\", \"value\"),\n",
    "    Output(\"current-df-meta\", \"data\"),\n",
    "    Input(\"dd-action\", \"value\"),\n",
    "    Input(\"dd-sensor\", \"value\"),\n",
    "    Input(\"dd-rec\", \"value\"),\n",
    ")\n",
    "def update_axes(action, sensor, idx):\n",
    "    if not action or not sensor or idx is None:\n",
    "        return [], [], None\n",
    "    try:\n",
    "        df = data[action][sensor][idx]\n",
    "        if df is None or df.empty:\n",
    "            return [], [], None\n",
    "        axes = available_axes(df)\n",
    "        opts = [{\"label\": a.upper(), \"value\": a} for a in axes]\n",
    "        return opts, axes, {\"action\": action, \"sensor\": sensor, \"idx\": idx}\n",
    "    except Exception:\n",
    "        return [], [], None\n",
    "\n",
    "# Remember zoom/pan; keep when changing controls\n",
    "@app.callback(\n",
    "    Output(\"xrange-store\", \"data\"),\n",
    "    Input(\"graph\", \"relayoutData\"),\n",
    "    State(\"xrange-store\", \"data\"),\n",
    "    prevent_initial_call=True,\n",
    ")\n",
    "def remember_zoom(relayout, stored):\n",
    "    if not relayout:\n",
    "        return no_update\n",
    "    if relayout.get(\"xaxis.autorange\"):\n",
    "        return None\n",
    "    x0 = relayout.get(\"xaxis.range[0]\") or (relayout.get(\"xaxis.range\", [None, None])[0] if \"xaxis.range\" in relayout else None)\n",
    "    x1 = relayout.get(\"xaxis.range[1]\") or (relayout.get(\"xaxis.range\", [None, None])[1] if \"xaxis.range\" in relayout else None)\n",
    "    if x0 is None or x1 is None:\n",
    "        return no_update\n",
    "    try:\n",
    "        return {\"x0\": float(x0), \"x1\": float(x1)}\n",
    "    except Exception:\n",
    "        return no_update\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"graph\", \"figure\"),\n",
    "    Input(\"current-df-meta\", \"data\"),\n",
    "    Input(\"cl-axes\", \"value\"),\n",
    "    Input(\"sl-downsample\", \"value\"),\n",
    "    Input(\"ri-showmode\", \"value\"),\n",
    "    Input(\"dd-smooth\", \"value\"),\n",
    "    Input(\"in-ma-sg-window\", \"value\"),\n",
    "    Input(\"in-sg-poly\", \"value\"),\n",
    "    Input(\"in-bw-cutoff\", \"value\"),\n",
    "    Input(\"in-bw-order\", \"value\"),\n",
    "    State(\"xrange-store\", \"data\"),\n",
    ")\n",
    "def update_graph(meta, axes_selected, ds, showmode,\n",
    "                 sm_method, ma_sg_window, sg_poly, bw_cutoff, bw_order,\n",
    "                 xr):\n",
    "    if not meta:\n",
    "        return go.Figure()\n",
    "\n",
    "    action = meta[\"action\"]; sensor = meta[\"sensor\"]; idx = meta[\"idx\"]\n",
    "    df = data[action][sensor][idx]\n",
    "    if df is None or df.empty or \"seconds_elapsed\" not in df.columns:\n",
    "        return go.Figure()\n",
    "\n",
    "    df = df.sort_values(\"seconds_elapsed\", kind=\"stable\").dropna()\n",
    "\n",
    "    # Downsample first (so smoothing runs on what you plot)\n",
    "    if isinstance(ds, int) and ds > 1:\n",
    "        df = df.iloc[::ds, :]\n",
    "\n",
    "    axes_all = available_axes(df)\n",
    "    if not axes_selected:\n",
    "        axes_selected = axes_all\n",
    "\n",
    "    # Build smoothing params\n",
    "    params = {\n",
    "        \"replace\": (showmode == \"sm\"),\n",
    "        \"ma_window\": int(ma_sg_window or 5),\n",
    "        \"sg_window\": int(ma_sg_window or 7),\n",
    "        \"sg_poly\": int(sg_poly or 3),\n",
    "        \"bw_cutoff\": float(bw_cutoff or 2.0),\n",
    "        \"bw_order\": int(bw_order or 4),\n",
    "    }\n",
    "\n",
    "    # Apply smoothing (adds *_sm columns if not replacing)\n",
    "    df_sm = apply_smoothing(df, axes_selected, sm_method or \"none\", params)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Decide which traces to draw\n",
    "    draw_raw = showmode in (\"raw\", \"overlay\")\n",
    "    draw_sm  = (showmode in (\"sm\", \"overlay\")) and (sm_method != \"none\")\n",
    "\n",
    "    for a in axes_selected:\n",
    "        if draw_raw and a in df_sm.columns:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df_sm[\"seconds_elapsed\"], y=df_sm[a],\n",
    "                mode=\"lines\", name=a.upper()+\" (raw)\",\n",
    "                line=dict(dash=\"dot\"),\n",
    "                hovertemplate=\"t=%{x:.3f}s<br>%{y:.5f}<extra>\"+a.upper()+\" raw</extra>\"\n",
    "            ))\n",
    "        if draw_sm:\n",
    "            sm_col = a if params[\"replace\"] else f\"{a}_sm\"\n",
    "            if sm_col in df_sm.columns:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=df_sm[\"seconds_elapsed\"], y=df_sm[sm_col],\n",
    "                    mode=\"lines\", name=a.upper()+\" (smoothed)\",\n",
    "                    hovertemplate=\"t=%{x:.3f}s<br>%{y:.5f}<extra>\"+a.upper()+\" sm</extra>\"\n",
    "                ))\n",
    "\n",
    "    # If smoothing is 'none' but user chose \"Smoothed only\", fall back to raw\n",
    "    if sm_method == \"none\" and showmode == \"sm\":\n",
    "        for a in axes_selected:\n",
    "            if a in df_sm.columns:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=df_sm[\"seconds_elapsed\"], y=df_sm[a],\n",
    "                    mode=\"lines\", name=a.upper(),\n",
    "                ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Time (s)\",\n",
    "        yaxis_title=\"Value\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"left\", x=0),\n",
    "        margin=dict(l=40, r=20, t=30, b=40),\n",
    "        uirevision=\"keep-zoom\",  # preserve zoom/pan across updates\n",
    "    )\n",
    "    fig.update_xaxes(rangeslider=dict(visible=True))\n",
    "\n",
    "    # Re-apply stored range if present\n",
    "    if xr and all(k in xr for k in (\"x0\", \"x1\")):\n",
    "        fig.update_xaxes(range=[xr[\"x0\"], xr[\"x1\"]])\n",
    "\n",
    "    return fig\n",
    "\n",
    "# ---------- Launch (works in Jupyter) ----------\n",
    "PORT = 8050\n",
    "app.run(host=\"127.0.0.1\", port=PORT, debug=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0958130",
   "metadata": {},
   "source": [
    "## Create Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97d8c9",
   "metadata": {},
   "source": [
    "After preparing and pre-processing the data, the dataset now must be split into samples, i.e. small timeframes of the total dataset. This will allow us to later assign a number of these to a training set, while the rest will be used for testing.\n",
    "\n",
    "The function presented below will be used later to extract these samples from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f466d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_into_samples(data: pd.DataFrame, window_s: float = 5.0) -> List[pd.DataFrame]:\n",
    "    \n",
    "    # Identify end time (last timestamp in data recording)\n",
    "    t_end = float(data[\"seconds_elapsed\"].iloc[-1])\n",
    "\n",
    "    out: List[pd.DataFrame] = []\n",
    "\n",
    "    # Call start (time = 0s)\n",
    "    start = 0.0\n",
    "\n",
    "    # Loop until end of recording\n",
    "    while start + window_s <= t_end + 1e-9:\n",
    "\n",
    "        # End of current sample\n",
    "        end = start + window_s\n",
    "\n",
    "        # Extract the rows in the current sample window\n",
    "        mask = (data[\"seconds_elapsed\"] >= start) & (data[\"seconds_elapsed\"] < end)\n",
    "        chunk = data.loc[mask].copy()\n",
    "\n",
    "        # Reset start time to t = 0s\n",
    "        chunk[\"seconds_elapsed\"] = chunk[\"seconds_elapsed\"] - chunk[\"seconds_elapsed\"].iloc[0]\n",
    "        out.append(chunk.reset_index(drop=True))\n",
    "\n",
    "        # Continue to next sample window\n",
    "        start = end\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaab9f66",
   "metadata": {},
   "source": [
    "Similarly to some earlier functions in this notebook, the function above must be applied to each axis, of each sensor, for each action. To do this, the function below again loops over these to make sure that this happens for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0448cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_samples(data: Dict[str, Dict[str, List[pd.DataFrame]]]) -> Dict[str, Dict[str, List[pd.DataFrame]]]:\n",
    "    \"\"\"\n",
    "    Convert data into samples:\n",
    "        samples[action][sensor] = list of 5s DataFrames\n",
    "    \"\"\"\n",
    "    samples: Dict[str, Dict[str, List[pd.DataFrame]]] = {}\n",
    "    for action, sensors in data.items():\n",
    "        if action != 'sittingDown+StandingUp':\n",
    "            samples[action] = {}\n",
    "        else:\n",
    "            samples['sittingDown'] = {}\n",
    "            samples['standingUp'] = {}\n",
    "        for sensor, recordings in sensors.items():\n",
    "            if action != 'sittingDown+StandingUp':\n",
    "                sensor_samples: List[pd.DataFrame] = []\n",
    "                for rec_df in recordings:\n",
    "                    split_samples = _split_into_samples(rec_df)\n",
    "                    sensor_samples.extend(split_samples)\n",
    "                samples[action][sensor] = sensor_samples\n",
    "            else:\n",
    "                sitting_down_samples: List[pd.DataFrame] = []\n",
    "                standing_up_samples: List[pd.DataFrame] = []\n",
    "                for rec_df in recordings:\n",
    "                    split_samples = _split_into_samples(rec_df)\n",
    "                    sitting_down_samples.extend(split_samples[0::2])\n",
    "                    standing_up_samples.extend(split_samples[1::2])\n",
    "                samples['sittingDown'][sensor] = sitting_down_samples\n",
    "                samples['standingUp'][sensor] = standing_up_samples\n",
    "    return samples\n",
    "\n",
    "samples = build_samples(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de40c0",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b42881d",
   "metadata": {},
   "source": [
    "With the data now extracted into samples, the total sample database can be split into a training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e9c3de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_samples(\n",
    "    samples: Dict[str, Dict[str, List[pd.DataFrame]]],\n",
    "    train_ratio: float,\n",
    "    seed: int = 42,\n",
    ") -> Tuple[Dict[str, Dict[str, List[pd.DataFrame]]],\n",
    "           Dict[str, Dict[str, List[pd.DataFrame]]],\n",
    "           Dict[str, Dict[str, List[pd.DataFrame]]]]:\n",
    "    \n",
    "    def _init_empty_object(src):\n",
    "        return {action: {sensor: [] for sensor in sensors}\n",
    "            for action, sensors in src.items()}\n",
    "    \n",
    "    train = _init_empty_object(samples)\n",
    "    test  = _init_empty_object(samples)\n",
    "    \n",
    "    random_number = random.Random(seed)\n",
    "\n",
    "    for action in samples.keys():\n",
    "        for sensor in samples[action].keys():\n",
    "            sample_list = samples[action][sensor] \n",
    "            n = len(sample_list)\n",
    "\n",
    "            sample_nums = list(range(n))\n",
    "            random_number.shuffle(sample_nums)\n",
    "\n",
    "            # Compute split sizes (ensure they sum to n)\n",
    "            n_train = int(n * train_ratio)\n",
    "\n",
    "            sample_nums_train = sample_nums[:n_train]\n",
    "            sample_nums_test  = sample_nums[n_train:]\n",
    "\n",
    "\n",
    "            train[action][sensor] = [sample_list[i] for i in sample_nums_train]\n",
    "            test[action][sensor]  = [sample_list[i] for i in sample_nums_test]\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train_samples, test_samples = split_samples(samples, 0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea25eb9",
   "metadata": {},
   "source": [
    "## Visualize Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2f982720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x15bfb276810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# app_samples_viewer.py\n",
    "import os\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "from dash import Dash, dcc, html, Input, Output, State, ctx, no_update\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# ---------- IMPORTANT ----------\n",
    "# Expect these to be available in the global scope if you've split already:\n",
    "#   train_samples, val_samples, test_samples\n",
    "# If not present, the app will fall back to `samples`.\n",
    "try:\n",
    "    samples  # type: ignore # noqa: F821\n",
    "except NameError:\n",
    "    # Dummy fallback to avoid NameError if you run this file standalone.\n",
    "    # Replace this with your real data before running.\n",
    "    samples = {\"walking\": {\"Accelerometer\": []}}\n",
    "\n",
    "# Helper: return the dict for a chosen dataset name\n",
    "def get_dataset_dict(dataset_name: str) -> Dict[str, Dict[str, List[pd.DataFrame]]]:\n",
    "    # Prefer explicitly split sets if available; otherwise fallback to `samples`\n",
    "    if dataset_name == \"train\" and \"train_samples\" in globals():\n",
    "        return globals()[\"train_samples\"]\n",
    "    if dataset_name == \"val\" and \"val_samples\" in globals():\n",
    "        return globals()[\"val_samples\"]\n",
    "    if dataset_name == \"test\" and \"test_samples\" in globals():\n",
    "        return globals()[\"test_samples\"]\n",
    "    # Fallback (treat whole thing as \"all\")\n",
    "    return samples\n",
    "\n",
    "def get_actions(dataset_name: str) -> List[str]:\n",
    "    ds = get_dataset_dict(dataset_name)\n",
    "    return list(ds.keys())\n",
    "\n",
    "def get_sensors(dataset_name: str, action: str) -> List[str]:\n",
    "    ds = get_dataset_dict(dataset_name)\n",
    "    return list(ds.get(action, {}).keys())\n",
    "\n",
    "def get_sample_count(dataset_name: str, action: str, sensor: str) -> int:\n",
    "    ds = get_dataset_dict(dataset_name)\n",
    "    return len(ds.get(action, {}).get(sensor, []))\n",
    "\n",
    "def make_sample_options(dataset_name: str, action: str, sensor: str, max_options: int = 10000):\n",
    "    n = get_sample_count(dataset_name, action, sensor)\n",
    "    return [{\"label\": f\"Sample {i}\", \"value\": i} for i in range(min(n, max_options))]\n",
    "\n",
    "app = Dash(__name__)\n",
    "server = app.server\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H2(\"Sample Browser\"),\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.Label(\"Dataset\"),\n",
    "            dcc.Dropdown(\n",
    "                id=\"ddl-dataset\",\n",
    "                options=[\n",
    "                    {\"label\": \"Train\", \"value\": \"train\"},\n",
    "                    {\"label\": \"Validation\", \"value\": \"val\"},\n",
    "                    {\"label\": \"Test\", \"value\": \"test\"},\n",
    "                    {\"label\": \"All (fallback to samples)\", \"value\": \"all\"},\n",
    "                ],\n",
    "                value=\"train\" if \"train_samples\" in globals() else (\"all\"),\n",
    "                clearable=False,\n",
    "            ),\n",
    "        ], style={\"width\": \"18%\"}),\n",
    "\n",
    "        html.Div([\n",
    "            html.Label(\"Action\"),\n",
    "            dcc.Dropdown(\n",
    "                id=\"ddl-action\",\n",
    "                options=[],\n",
    "                value=None,\n",
    "                clearable=False,\n",
    "            ),\n",
    "        ], style={\"width\": \"22%\"}),\n",
    "\n",
    "        html.Div([\n",
    "            html.Label(\"Sensor\"),\n",
    "            dcc.Dropdown(\n",
    "                id=\"ddl-sensor\",\n",
    "                options=[],\n",
    "                value=None,\n",
    "                clearable=False,\n",
    "            ),\n",
    "        ], style={\"width\": \"22%\"}),\n",
    "\n",
    "        html.Div([\n",
    "            html.Label(\"Samples\"),\n",
    "            dcc.Dropdown(\n",
    "                id=\"ddl-samples\",\n",
    "                options=[],\n",
    "                value=[],\n",
    "                multi=True,\n",
    "                placeholder=\"Select one or more samples\",\n",
    "            ),\n",
    "        ], style={\"width\": \"28%\"}),\n",
    "\n",
    "        html.Div([\n",
    "            html.Label(\"View Mode\"),\n",
    "            dcc.RadioItems(\n",
    "                id=\"rad-mode\",\n",
    "                options=[\n",
    "                    {\"label\": \"Overlay\", \"value\": \"overlay\"},\n",
    "                    {\"label\": \"Separate\", \"value\": \"separate\"},\n",
    "                ],\n",
    "                value=\"overlay\",\n",
    "                inline=True,\n",
    "            ),\n",
    "        ], style={\"width\": \"10%\"}),\n",
    "    ], style={\"display\": \"flex\", \"gap\": \"1rem\", \"flexWrap\": \"wrap\", \"alignItems\": \"end\"}),\n",
    "\n",
    "    html.Div([\n",
    "        dcc.Checklist(\n",
    "            id=\"chk-axes\",\n",
    "            options=[{\"label\": \"x\", \"value\": \"x\"},\n",
    "                     {\"label\": \"y\", \"value\": \"y\"},\n",
    "                     {\"label\": \"z\", \"value\": \"z\"}],\n",
    "            value=[\"x\", \"y\", \"z\"],\n",
    "            inline=True\n",
    "        ),\n",
    "        html.Span(\"  (toggle axes)\", style={\"marginLeft\": \"0.5rem\", \"color\": \"#666\"})\n",
    "    ], style={\"margin\": \"0.5rem 0 1rem 0\"}),\n",
    "\n",
    "    dcc.Loading(\n",
    "        dcc.Graph(id=\"graph\", figure=go.Figure()),\n",
    "        type=\"default\"\n",
    "    ),\n",
    "], style={\"maxWidth\": \"1200px\", \"margin\": \"1.5rem auto\", \"fontFamily\": \"sans-serif\"})\n",
    "\n",
    "# --- Callbacks ---\n",
    "\n",
    "# Populate actions based on dataset, and persist selection if valid\n",
    "@app.callback(\n",
    "    Output(\"ddl-action\", \"options\"),\n",
    "    Output(\"ddl-action\", \"value\"),\n",
    "    Input(\"ddl-dataset\", \"value\"),\n",
    "    State(\"ddl-action\", \"value\"),\n",
    ")\n",
    "def on_dataset_change(dataset_name, current_action):\n",
    "    actions = get_actions(dataset_name or \"all\")\n",
    "    opts = [{\"label\": a, \"value\": a} for a in actions]\n",
    "    if not actions:\n",
    "        return [], None\n",
    "    value = current_action if current_action in actions else actions[0]\n",
    "    return opts, value\n",
    "\n",
    "# Keep sensor selection when action or dataset changes (if still valid)\n",
    "@app.callback(\n",
    "    Output(\"ddl-sensor\", \"options\"),\n",
    "    Output(\"ddl-sensor\", \"value\"),\n",
    "    Input(\"ddl-dataset\", \"value\"),\n",
    "    Input(\"ddl-action\", \"value\"),\n",
    "    State(\"ddl-sensor\", \"value\"),\n",
    ")\n",
    "def on_action_change(dataset_name, action, current_sensor):\n",
    "    if not dataset_name or not action:\n",
    "        return [], None\n",
    "    sensors = get_sensors(dataset_name, action)\n",
    "    opts = [{\"label\": s, \"value\": s} for s in sensors]\n",
    "    if not sensors:\n",
    "        return [], None\n",
    "    value = current_sensor if current_sensor in sensors else sensors[0]\n",
    "    return opts, value\n",
    "\n",
    "# Keep sample selection when sensor/action/dataset changes (if still valid)\n",
    "@app.callback(\n",
    "    Output(\"ddl-samples\", \"options\"),\n",
    "    Output(\"ddl-samples\", \"value\"),\n",
    "    Input(\"ddl-dataset\", \"value\"),\n",
    "    Input(\"ddl-action\", \"value\"),\n",
    "    Input(\"ddl-sensor\", \"value\"),\n",
    "    State(\"ddl-samples\", \"value\"),\n",
    ")\n",
    "def on_sensor_change(dataset_name, action, sensor, current_samples):\n",
    "    if not dataset_name or not action or not sensor:\n",
    "        return [], []\n",
    "    opts = make_sample_options(dataset_name, action, sensor)\n",
    "    valid = {o[\"value\"] for o in opts}\n",
    "    kept = [v for v in (current_samples or []) if v in valid]\n",
    "    # auto-pick first if nothing selected\n",
    "    if not kept and opts:\n",
    "        kept = [opts[0][\"value\"]]\n",
    "    return opts, kept\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"graph\", \"figure\"),\n",
    "    Input(\"ddl-dataset\", \"value\"),\n",
    "    Input(\"ddl-action\", \"value\"),\n",
    "    Input(\"ddl-sensor\", \"value\"),\n",
    "    Input(\"ddl-samples\", \"value\"),\n",
    "    Input(\"rad-mode\", \"value\"),\n",
    "    Input(\"chk-axes\", \"value\"),\n",
    ")\n",
    "def update_plot(dataset_name, action, sensor, selected_samples, mode, axes_on):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    if not dataset_name or not action or not sensor or not selected_samples:\n",
    "        fig.update_layout(\n",
    "            title=\"Select dataset, action, sensor, and samples\",\n",
    "            xaxis_title=\"Time (s)\",\n",
    "            yaxis_title=\"Value\",\n",
    "            template=\"plotly_white\",\n",
    "            height=550,\n",
    "        )\n",
    "        return fig\n",
    "\n",
    "    ds = get_dataset_dict(dataset_name if dataset_name != \"all\" else \"all\")\n",
    "    rec_samples = ds.get(action, {}).get(sensor, [])\n",
    "\n",
    "    def traces_for(df, prefix):\n",
    "        traces = []\n",
    "        for axis in [\"x\", \"y\", \"z\"]:\n",
    "            if axis in axes_on and axis in df.columns:\n",
    "                traces.append(go.Scatter(\n",
    "                    x=df[\"seconds_elapsed\"],\n",
    "                    y=df[axis],\n",
    "                    mode=\"lines\",\n",
    "                    name=f\"{prefix}{axis}\",\n",
    "                    hovertemplate=\"t=%{x:.3f}s<br>\"+f\"{axis}=%{{y:.3f}}<extra></extra>\"\n",
    "                ))\n",
    "        return traces\n",
    "\n",
    "    if mode == \"overlay\":\n",
    "        for idx in selected_samples:\n",
    "            if 0 <= idx < len(rec_samples):\n",
    "                df = rec_samples[idx]\n",
    "                fig.add_traces(traces_for(df, prefix=f\"#{idx} \"))\n",
    "        fig.update_layout(\n",
    "            title=f\"[{dataset_name}] {action} — {sensor} — overlay {len(selected_samples)} sample(s)\",\n",
    "            xaxis_title=\"Time (s)\",\n",
    "            yaxis_title=\"Value\",\n",
    "            template=\"plotly_white\",\n",
    "            height=650,\n",
    "            legend={\"orientation\": \"h\", \"yanchor\": \"bottom\", \"y\": 1.02, \"x\": 0.01},\n",
    "        )\n",
    "    else:\n",
    "        rows = len(selected_samples)\n",
    "        domains = []\n",
    "        gap = 0.04\n",
    "        panel = (1.0 - (rows - 1) * gap) / rows\n",
    "        for r in range(rows):\n",
    "            top = 1.0 - r * (panel + gap)\n",
    "            bottom = top - panel\n",
    "            domains.append((bottom, top))\n",
    "\n",
    "        for r, idx in enumerate(selected_samples):\n",
    "            if 0 <= idx < len(rec_samples):\n",
    "                df = rec_samples[idx]\n",
    "                yaxis_name = \"yaxis\" if r == 0 else f\"yaxis{r+1}\"\n",
    "                for tr in traces_for(df, prefix=f\"#{idx} \"):\n",
    "                    tr.update(yaxis=f\"y{'' if r == 0 else r+1}\")\n",
    "                    fig.add_trace(tr)\n",
    "                fig.update_layout(**{\n",
    "                    yaxis_name: dict(domain=list(domains[r]), title=f\"Sample #{idx}\"),\n",
    "                })\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"[{dataset_name}] {action} — {sensor} — {rows} separate panel(s)\",\n",
    "            xaxis=dict(title=\"Time (s)\"),\n",
    "            template=\"plotly_white\",\n",
    "            height=max(350, 280 * rows),\n",
    "            legend={\"orientation\": \"h\", \"yanchor\": \"bottom\", \"y\": 1.02, \"x\": 0.01},\n",
    "            margin=dict(t=60, r=30, l=60, b=40)\n",
    "        )\n",
    "\n",
    "    return fig\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For Dash >= 2.0 use app.run, not app.run_server\n",
    "    host = os.environ.get(\"HOST\", \"127.0.0.1\")\n",
    "    port = int(os.environ.get(\"PORT\", \"8050\"))\n",
    "    app.run(host=host, port=port, debug=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a0077",
   "metadata": {},
   "source": [
    "## Prepare for training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "7326fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Toggle to turn fourier transformation on/off ---\n",
    "USE_FFT = True            # set to False to use simple time-domain means\n",
    "FFT_TARGET_LEN = 256      # resample each signal to this many points per 5s window\n",
    "FFT_N_KEEP     = 32       # number of low-frequency rFFT bins to keep (including DC)\n",
    "\n",
    "# --- Imports & utils ---\n",
    "\n",
    "\n",
    "\n",
    "_TIME_COL = \"seconds_elapsed\"\n",
    "\n",
    "def _resample_to_fixed_length(t: np.ndarray, x: np.ndarray, n: int) -> np.ndarray:\n",
    "    \"\"\"Linear interpolation to n points on [t0, t1]. Assumes t is increasing.\"\"\"\n",
    "    if len(x) == 1:\n",
    "        return np.repeat(x.astype(float), n)\n",
    "    t0, t1 = float(t[0]), float(t[-1])\n",
    "    tgt = np.linspace(t0, t1, num=n, endpoint=True)\n",
    "    return np.interp(tgt, t.astype(float), x.astype(float))\n",
    "\n",
    "def _trial_features_mean(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Time-domain aggregation (column means, excluding time).\"\"\"\n",
    "    cols = [c for c in df.columns if c != _TIME_COL]\n",
    "    if df.shape[0] == 1:\n",
    "        s = df[cols].iloc[0]\n",
    "    else:\n",
    "        s = df[cols].mean(axis=0)\n",
    "    s.index = pd.Index([f\"{c}\" for c in s.index])\n",
    "    return s\n",
    "\n",
    "def _trial_features_fft(\n",
    "    df: pd.DataFrame,\n",
    "    target_len: int = FFT_TARGET_LEN,\n",
    "    n_keep: int = FFT_N_KEEP\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    rFFT magnitude features per channel after resampling each channel to a fixed length.\n",
    "    Returns first n_keep bins (including DC). Columns that are non-numeric are skipped.\n",
    "    \"\"\"\n",
    "    assert _TIME_COL in df.columns, f\"Expected time column '{_TIME_COL}'.\"\n",
    "    t = df[_TIME_COL].to_numpy()\n",
    "    out_parts = []\n",
    "    for col in df.columns:\n",
    "        if col == _TIME_COL:\n",
    "            continue\n",
    "        x = pd.to_numeric(df[col], errors=\"coerce\").to_numpy()\n",
    "        if np.all(np.isnan(x)):\n",
    "            continue\n",
    "        x = np.nan_to_num(x, nan=np.nanmean(x) if np.isfinite(np.nanmean(x)) else 0.0)\n",
    "        xr = _resample_to_fixed_length(t, x, target_len)\n",
    "        # Real FFT, normalize by length for scale invariance\n",
    "        spec = np.fft.rfft(xr)\n",
    "        mag = np.abs(spec) / target_len\n",
    "        k = min(n_keep, mag.shape[0])\n",
    "        vals = mag[:k]\n",
    "        idx = pd.Index([f\"{col}__fft{k_}\" for k_ in range(k)])\n",
    "        out_parts.append(pd.Series(vals, index=idx))\n",
    "    if not out_parts:\n",
    "        # fallback to zeros if all channels invalid\n",
    "        return pd.Series(dtype=float)\n",
    "    return pd.concat(out_parts)\n",
    "\n",
    "def _aggregate_trial_df(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Select featureization mode.\"\"\"\n",
    "    return _trial_features_fft(df) if USE_FFT else _trial_features_mean(df)\n",
    "\n",
    "def _concat_sensors_for_trial(trial_per_sensor: Dict[str, pd.DataFrame]) -> pd.Series:\n",
    "    parts = []\n",
    "    for sensor, df in trial_per_sensor.items():\n",
    "        s = _aggregate_trial_df(df)\n",
    "        # Prefix with sensor name\n",
    "        s.index = pd.Index([f\"{sensor}__{c}\" for c in s.index])\n",
    "        parts.append(s)\n",
    "    return pd.concat(parts) if parts else pd.Series(dtype=float)\n",
    "\n",
    "def build_Xy(split_dict: Dict[str, Dict[str, List[pd.DataFrame]]]) -> Tuple[pd.DataFrame, np.ndarray, List[str]]:\n",
    "    rows, labels = [], []\n",
    "    for activity, sensors in split_dict.items():\n",
    "        first_sensor = next(iter(sensors))\n",
    "        n_trials = len(sensors[first_sensor])\n",
    "        for sensor, lst in sensors.items():\n",
    "            if len(lst) != n_trials:\n",
    "                raise ValueError(f\"Inconsistent trials for {activity}: {first_sensor}={n_trials}, {sensor}={len(lst)}\")\n",
    "        for j in range(n_trials):\n",
    "            trial_per_sensor = {s: sensors[s][j] for s in sensors.keys()}\n",
    "            rows.append(_concat_sensors_for_trial(trial_per_sensor))\n",
    "            labels.append(activity)\n",
    "    # Align columns across variable-length Series by outer join\n",
    "    X = pd.DataFrame(rows).fillna(0.0).reset_index(drop=True)\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(labels)\n",
    "    classes = list(le.classes_)\n",
    "    return X, y, classes\n",
    "\n",
    "def evaluate(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"kappa\": float(cohen_kappa_score(y_true, y_pred)),\n",
    "        \"f1_macro\": float(f1_score(y_true, y_pred, average=\"macro\")),\n",
    "    }\n",
    "\n",
    "def align_clusters(y_true: np.ndarray, y_cluster: np.ndarray, n_classes: int) -> Tuple[np.ndarray, Dict[int, int]]:\n",
    "    C = confusion_matrix(y_true, y_cluster, labels=np.arange(n_classes))\n",
    "    cost = C.max() - C\n",
    "    r, c = linear_sum_assignment(cost)\n",
    "    mapping = {cluster: cls for cluster, cls in zip(c, r)}\n",
    "    y_aligned = np.array([mapping[z] for z in y_cluster])\n",
    "    return y_aligned, mapping\n",
    "\n",
    "# --- Build matrices (now FFT-based if USE_FFT=True) ---\n",
    "X_train, y_train, classes = build_Xy(train_samples)\n",
    "X_val,   y_val,   _       = build_Xy(val_samples)\n",
    "X_test,  y_test,  _       = build_Xy(test_samples)\n",
    "n_classes = len(classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e207b5",
   "metadata": {},
   "source": [
    "# Supervised models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15da71ba",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "3c3a6c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'accuracy': 0.9916666666666667,\n",
       "  'kappa': 0.9895833333333334,\n",
       "  'f1_macro': 0.9916630481980026},\n",
       " {'accuracy': 0.9939393939393939,\n",
       "  'kappa': 0.9924242424242424,\n",
       "  'f1_macro': 0.9939380022962112})"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=0.95)),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "y_val_hat  = logreg.predict(X_val)\n",
    "y_test_hat = logreg.predict(X_test)\n",
    "\n",
    "logreg_val_metrics  = evaluate(y_val,  y_val_hat)\n",
    "logreg_test_metrics = evaluate(y_test, y_test_hat)\n",
    "logreg_val_metrics, logreg_test_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f14c2b8",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "54b666e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'accuracy': 0.9666666666666667,\n",
       "  'kappa': 0.9583333333333334,\n",
       "  'f1_macro': 0.9664857432334635},\n",
       " {'accuracy': 0.984, 'kappa': 0.98, 'f1_macro': 0.984})"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=0.95)),\n",
    "    (\"clf\", KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "y_val_hat  = knn.predict(X_val)\n",
    "y_test_hat = knn.predict(X_test)\n",
    "\n",
    "knn_val_metrics  = evaluate(y_val,  y_val_hat)\n",
    "knn_test_metrics = evaluate(y_test, y_test_hat)\n",
    "knn_val_metrics, knn_test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f51783c",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0e3c1316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'accuracy': 1.0, 'kappa': 1.0, 'f1_macro': 1.0},\n",
       " {'accuracy': 1.0, 'kappa': 1.0, 'f1_macro': 1.0})"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=0.95)),\n",
    "    (\"clf\", GaussianNB())\n",
    "])\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "y_val_hat  = gnb.predict(X_val)\n",
    "y_test_hat = gnb.predict(X_test)\n",
    "\n",
    "gnb_val_metrics  = evaluate(y_val,  y_val_hat)\n",
    "gnb_test_metrics = evaluate(y_test, y_test_hat)\n",
    "gnb_val_metrics, gnb_test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f86be",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "76a71a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'accuracy': 1.0, 'kappa': 1.0, 'f1_macro': 1.0},\n",
       " {'accuracy': 0.9555555555555556,\n",
       "  'kappa': 0.9444444444444444,\n",
       "  'f1_macro': 0.9564705882352941})"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_pipe = Pipeline([\n",
    "    (\"pca\", PCA(n_components=0.95)),\n",
    "    (\"clf\", DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "dt_pipe.fit(X_train, y_train)\n",
    "\n",
    "y_val_hat  = dt_pipe.predict(X_val)\n",
    "y_test_hat = dt_pipe.predict(X_test)\n",
    "\n",
    "dt_val_metrics  = evaluate(y_val,  y_val_hat)\n",
    "dt_test_metrics = evaluate(y_test, y_test_hat)\n",
    "dt_val_metrics, dt_test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69203425",
   "metadata": {},
   "source": [
    "# Unsupervised models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5162f2",
   "metadata": {},
   "source": [
    "## k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5648bfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7833333333333333,\n",
       " 'kappa': 0.7291666666666667,\n",
       " 'f1_macro': 0.7663945578231293}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=0.95)),\n",
    "    (\"kmeans\", KMeans(n_clusters=n_classes, n_init=\"auto\", random_state=42))\n",
    "])\n",
    "\n",
    "kmeans.fit(X_train)\n",
    "k_train = kmeans[-1].labels_\n",
    "k_val   = kmeans.predict(X_val)  # uses scaler + kmeans\n",
    "\n",
    "y_train_aligned, map_k = align_clusters(y_train, k_train, n_classes)\n",
    "y_val_aligned = np.array([map_k[z] for z in k_val])\n",
    "\n",
    "kmeans_val_metrics = evaluate(y_val, y_val_aligned)\n",
    "kmeans_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f222dc",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ee608e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6583333333333333,\n",
       " 'kappa': 0.5729166666666667,\n",
       " 'f1_macro': 0.6175236277581693}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=0.3)),\n",
    "    (\"gmm\", GaussianMixture(n_components=n_classes, \n",
    "                            covariance_type=\"full\", \n",
    "                            random_state=42))\n",
    "])\n",
    "\n",
    "gmm_pipe.fit(X_train)\n",
    "g_train = gmm_pipe.predict(X_train)\n",
    "g_val   = gmm_pipe.predict(X_val)\n",
    "\n",
    "\n",
    "\n",
    "y_train_aligned, map_g = align_clusters(y_train, g_train, n_classes)\n",
    "y_val_aligned = np.array([map_g[z] for z in g_val])\n",
    "\n",
    "gmm_val_metrics = evaluate(y_val, y_val_aligned)\n",
    "gmm_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18548150",
   "metadata": {},
   "source": [
    "## Fuzzy C-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5445ea6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8, 'kappa': 0.75, 'f1_macro': 0.7333333333333333}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scale then FCM (skfuzzy expects features x samples)\n",
    "scaler = StandardScaler().fit(X_train.values)\n",
    "X_train_scaled = scaler.transform(X_train.values)  # d x n\n",
    "X_val_scaled = scaler.transform(X_val.values)\n",
    "\n",
    "pca = PCA(n_components=0.90)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_val_pca   = pca.transform(X_val_scaled)\n",
    "\n",
    "# FCM expects features x samples\n",
    "Xtr = X_train_pca.T\n",
    "Xva = X_val_pca.T\n",
    "\n",
    "cntr, u_tr, _, _, _, _, _ = fuzz.cluster.cmeans(\n",
    "    data=Xtr, c=n_classes, m=2.0, error=1e-5, maxiter=1000, seed=42\n",
    ")\n",
    "f_train = np.argmax(u_tr, axis=0)\n",
    "\n",
    "# align clusters\n",
    "y_train_aligned, map_f = align_clusters(y_train, f_train, n_classes)\n",
    "\n",
    "# predict on validation\n",
    "u_va, _, _, _, _, _ = fuzz.cluster.cmeans_predict(\n",
    "    test_data=Xva, cntr_trained=cntr, m=2.0, error=1e-8, maxiter=50\n",
    ")\n",
    "f_val = np.argmax(u_va, axis=0)\n",
    "y_val_aligned = np.array([map_f[z] for z in f_val])\n",
    "\n",
    "fcm_val_metrics = evaluate(y_val, y_val_aligned)\n",
    "fcm_val_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
